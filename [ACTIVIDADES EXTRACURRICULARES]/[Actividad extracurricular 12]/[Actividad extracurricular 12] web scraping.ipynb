{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Actividad extracurricular 12] Web scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "\n",
    "El **web scraping** es una técnica automatizada para extraer datos y contenido de sitios web. Esta herramienta resulta especialmente útil cuando se requiere recopilar grandes volúmenes de información de forma rápida y eficiente.  \n",
    "Se trata de un conjunto de métodos que permiten obtener datos directamente de la web sin intervención manual. Sus aplicaciones son variadas e incluyen:  \n",
    "\n",
    "- Investigación de mercado.  \n",
    "- Comparación de precios.  \n",
    "- Supervisión de contenido.  \n",
    "- Recolección de datos de diversas fuentes en línea.  \n",
    "\n",
    "# Funcionamiento\n",
    "\n",
    "El web scraping se lleva a cabo mediante herramientas y librerías especializadas en distintos lenguajes de programación. Python es uno de los lenguajes más populares en este campo gracias a librerías como **BeautifulSoup** y **Scrapy**.  \n",
    "\n",
    "Con esta técnica, es posible extraer datos desde diversas fuentes:  \n",
    "\n",
    "- Motores de búsqueda.  \n",
    "- Feeds RSS.  \n",
    "- Sitios web gubernamentales.  \n",
    "\n",
    "Aunque muchos sitios permiten el acceso automatizado mediante scrapers o crawlers, algunos imponen restricciones que pueden requerir métodos o herramientas adicionales.  \n",
    "Por ejemplo:  \n",
    "\n",
    "- Extraer información de contenido visual, como imágenes, puede ser más complejo.  \n",
    "- Obtener datos de elementos dinámicos que dependen de JavaScript suele demandar técnicas avanzadas.  \n",
    "\n",
    "# Consideraciones legales y éticas\n",
    "\n",
    "Al realizar web scraping, es crucial respetar las normativas legales y éticas. Muchos sitios web especifican sus políticas sobre el acceso automatizado a través del archivo **robots.txt**.  \n",
    "\n",
    "Ignorar estas directrices puede generar:  \n",
    "\n",
    "- Sanciones legales.  \n",
    "- Restricción del acceso al sitio web.  \n",
    "\n",
    "Por lo tanto, es importante actuar de manera responsable y asegurarse de cumplir con las reglas establecidas para evitar problemas legales y éticos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando BeatifulSoup\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# URL del sitio web\n",
    "url = 'https://www.epn.edu.ec'\n",
    "# Realizamos la solicitud HTTP\n",
    "response = requests.get(url)\n",
    "# Si la solicitud es exitosa (status 200), continuamos\n",
    "if response.status_code == 200:\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "# Extraemos los títulos del sitio\n",
    "titles = soup.find_all('h1') # Busca todos los <h1> en la página\n",
    "for title in titles:\n",
    "print(title.text)\n",
    "# Usando Scrapy\n",
    "from requests_html import HTMLSession\n",
    "# Crear una sesión de requests HTML\n",
    "session = HTMLSession()\n",
    "# URL del sitio web\n",
    "url = 'https://www.epn.edu.ec'\n",
    "# Realizar la solicitud HTTP al sitio web\n",
    "response = session.get(url)\n",
    "# Extraer todos los títulos de los encabezados <h1>\n",
    "for title in response.html.find('h1'):\n",
    "print({'Title': title.text})\n",
    "# Cerrar la sesión cuando termine\n",
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
